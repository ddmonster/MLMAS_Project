{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c714efc4",
   "metadata": {},
   "source": [
    "# The Utility code\n",
    "## This code helps in analysing the scenarios record and display the required results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c5974",
   "metadata": {},
   "source": [
    "### CARLA Location and Connection information\n",
    ">#### Note: The Jupyter notebook should be run from the mlmas environment.\n",
    ">#### Make sure that the CARLA simulator is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc3db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the carla location and IP to connect with.\n",
    "import os\n",
    "\n",
    "prj_dir = os.getcwd().replace('/MLMAS_Framework/tools','')\n",
    "\n",
    "CARLA_PATH = prj_dir+'/CARLA_0.9.10.1'\n",
    "\n",
    "\n",
    "HOST = '172.21.144.1'\n",
    "PORT = 2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de0eeb",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c408f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code and make sure CARLA simulator is running\n",
    "\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "try:\n",
    "    sys.path.append(glob.glob(CARLA_PATH+'/PythonAPI/carla/dist/carla-0.9.10-py3.7-linux-x86_64.egg')[0])\n",
    "except IndexError:\n",
    "    print(\"error >>>\")\n",
    "\n",
    "import carla\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a3a54",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "### Record file path to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6344912",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder_filename = prj_dir+'/results/MLMAS_LAV_results.json'\n",
    "recorder_filename2 = prj_dir+'/results/records/MLMAS_tfus/RouteScenario_8_rep0.log'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84bd264",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/ddmonster/MLMAS_Project/results/records/tfus/RouteScenario_8_rep0.log not found on server\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TIME = 30\n",
    "DISTANCE = 10\n",
    "try:\n",
    "\n",
    "    client = carla.Client(HOST, PORT)\n",
    "    client.set_timeout(60.0)\n",
    "\n",
    "    print(client.show_recorder_actors_blocked(recorder_filename, TIME, DISTANCE))\n",
    "\n",
    "finally:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4d528",
   "metadata": {},
   "source": [
    "## 2. Analysing the Record Logs\n",
    "\n",
    "### - List of the possible blocked agents cases in this scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d6493",
   "metadata": {},
   "source": [
    "### - List of all collission cases in this scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97aa8bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    client = carla.Client(HOST, PORT)\n",
    "    client.set_timeout(60.0)\n",
    "\n",
    "    print(client.show_recorder_collisions(recorder_filename, \"v\", \"a\"))\n",
    "\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b64521",
   "metadata": {},
   "source": [
    "## 3. Run the Scenario in a Specific Period\n",
    "### - Baseed on the above code information specify the following:\n",
    "in the replay_file method replay_file(record_filename, start_sec, end_sec, camera_actor_id):\n",
    "- start_sec: From which second you want to start the scenario (eg. you want to start 3 sec before the collision happen based on the previous code information)\n",
    "- duration: Which second you want the scenario to stop.\n",
    "- camera_actor_id: based on the previous code information, you can get the id of the vehicle you want to follow, and specify it in this argument. however, if the id is invalud, the camera will be always shows from top. (By default configured to get the id of the hero-car that controlled by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the start time and the duration for the scenario to rerun in the specify period.\n",
    "Start_Time = 154\n",
    "Duration = 3000\n",
    "p = \"2\"\n",
    "def get_hero_vehicle(client):\n",
    "    try:\n",
    "        act_list = client.show_recorder_file_info(recorder_filename, False).splitlines()\n",
    "        \n",
    "        for i in range(len(act_list)):\n",
    "                if \"role_name = hero\" in act_list[i]:\n",
    "                    s = act_list[i-5]\n",
    "                    s = s[7:s.find(':')]\n",
    "                    return int(s)\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "try:\n",
    "    \n",
    "    client = carla.Client(HOST, PORT)\n",
    "    client.set_timeout(60.0)\n",
    "    client.reload_world()\n",
    "    client.replay_file(recorder_filename,\n",
    "                       time_start=Start_Time,\n",
    "                       duration=Duration,\n",
    "                       follow_id = get_hero_vehicle(client))\n",
    "\n",
    "finally:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the start time and the duration for the scenario to rerun in the specify period.\n",
    "Start_Time = 230\n",
    "Duration = 3000\n",
    "p = \"2\"\n",
    "def get_hero_vehicle(client):\n",
    "    try:\n",
    "        act_list = client.show_recorder_file_info(recorder_filename2, False).splitlines()\n",
    "        \n",
    "        for i in range(len(act_list)):\n",
    "                if \"role_name = hero\" in act_list[i]:\n",
    "                    s = act_list[i-5]\n",
    "                    s = s[7:s.find(':')]\n",
    "                    return int(s)\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "try:\n",
    "    \n",
    "    client = carla.Client(HOST, PORT)\n",
    "    client.set_timeout(60.0)\n",
    "    client.reload_world()\n",
    "    client.replay_file(recorder_filename2,\n",
    "                       time_start=Start_Time,\n",
    "                       duration=Duration,\n",
    "                       follow_id = get_hero_vehicle(client))\n",
    "\n",
    "finally:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a10499",
   "metadata": {},
   "outputs": [],
   "source": [
    "### =================== End Of Scenario Runner ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343e4a9",
   "metadata": {},
   "source": [
    "# Results Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb32cdb",
   "metadata": {},
   "source": [
    "## 1. configuration\n",
    "### Configure of the results files location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2bc5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmas_leaderboard_results_file = os.getcwd().replace('/MLMAS_Framework/tools','/results/MLMAS_tfus_results.json')\n",
    "mlmas_additional_metrics_file = os.getcwd().replace('/MLMAS_Framework/tools','/results/MLMAS_tfus_results_jason_metrics.csv')\n",
    "lav_leaderboard_results = os.getcwd().replace('/MLMAS_Framework/tools','/results/transfuser_longest6.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446c7a0",
   "metadata": {},
   "source": [
    "### Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01220cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLeaderboardJson(filepath):\n",
    "    f = open(os.path.realpath(filepath))\n",
    "    return json.load(f)\n",
    "    \n",
    "\n",
    "def geLeaderboardMetrics(jsn):\n",
    "    leaderboard_metrics = jsn['values']\n",
    "    leaderboard_df = pd.DataFrame([jsn['values']],\n",
    "                                  columns=jsn['labels'])\n",
    "    leaderboard_df = leaderboard_df.astype(float)\n",
    "    \n",
    "    return leaderboard_df\n",
    "\n",
    "\n",
    "\n",
    "mlmas_leaderboard_jsn = loadLeaderboardJson(mlmas_leaderboard_results_file)\n",
    "lav_leaderboard_jsn = loadLeaderboardJson(lav_leaderboard_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcf07e",
   "metadata": {},
   "source": [
    "## 2. Display the leaderboard results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmas_leaderboard_df = geLeaderboardMetrics(mlmas_leaderboard_jsn)\n",
    "lav_leaderboard_df = geLeaderboardMetrics(lav_leaderboard_jsn)\n",
    "display(mlmas_leaderboard_df)\n",
    "print(\"== MLMAS Framework with LAV Model Leaderboard Metrics results ==\")\n",
    "print(\"== ========================================================== ==\")\n",
    "display(lav_leaderboard_df)\n",
    "print(\"== LAV Model Only Leaderboard Metrics results ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeTwoLeaderbordMetricsResults(title1, title2, df1, df2):\n",
    "    score_columns = df1.columns.values[0:3]\n",
    "    df_s1 = df1[score_columns]\n",
    "    df_s1.loc[:,2:3] = df_s1[score_columns[2]].values*100\n",
    "    df_s2 = df2[score_columns]\n",
    "    df_s2.loc[:,2:3] = df_s2[score_columns[2]].values*100\n",
    "    \n",
    "    collisions_columns = df1.columns.values[3:6]\n",
    "    \n",
    "    road_columns = df1.columns.values[6:9]\n",
    "    \n",
    "    other_columns = df1.columns.values[9:]\n",
    "    \n",
    "    final_df_s = pd.DataFrame({title1: df_s1.values[0]\n",
    "                  , title2: df_s2.values[0]\n",
    "                  , 'Results_Higher_is_Better': score_columns}) \n",
    "    \n",
    "    final_df_c = pd.DataFrame({title1: df1[collisions_columns].values[0]\n",
    "          , title2: df2[collisions_columns].values[0]\n",
    "          , 'Results_Lower_is_Better': collisions_columns}) \n",
    "    \n",
    "    final_df_r = pd.DataFrame({title1: df1[road_columns].values[0]\n",
    "              , title2: df2[road_columns].values[0]\n",
    "              , 'Results_Lower_is_Better': road_columns}) \n",
    "    \n",
    "    final_df_o = pd.DataFrame({title1: df1[other_columns].values[0]\n",
    "          , title2: df2[other_columns].values[0]\n",
    "          , 'Results_Lower_is_Better': other_columns}) \n",
    "        \n",
    "        \n",
    "    final_df_s.index = final_df_s.Results_Higher_is_Better\n",
    "    final_df_c.index = final_df_c.Results_Lower_is_Better\n",
    "    final_df_o.index = final_df_o.Results_Lower_is_Better\n",
    "    final_df_r.index = final_df_r.Results_Lower_is_Better\n",
    "    return final_df_s, final_df_c, final_df_o, final_df_r\n",
    "\n",
    "def show_values(g, pr=\"%\", is_vertical = True):\n",
    "    for p in g.patches:\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        x, y = p.get_xy() \n",
    "        if is_vertical:\n",
    "            g.annotate(f'{round(height,1)}{pr}' if pr == \"%\" else f'{height}{pr}', (x + width/2, y + height*1.02), ha='center')\n",
    "        else:\n",
    "            g.annotate(f'{width}{pr}', (x + width + 0.09, y + height/2), ha='center')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b07b5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 17}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "\n",
    "final_df_s, final_df_c, final_df_o, final_df_r = mergeTwoLeaderbordMetricsResults('transfuser','ML-MAS', lav_leaderboard_df ,mlmas_leaderboard_df)\n",
    "graph = final_df_s.plot.bar(figsize=(13,8))\n",
    "\n",
    "graph2 = final_df_c.plot.bar(figsize=(13,8))\n",
    "graph3 = final_df_r.plot.bar(figsize=(13,8))\n",
    "graph4 = final_df_o.plot.bar(figsize=(11,9))\n",
    "\n",
    "plt.tight_layout()\n",
    "show_values(graph)\n",
    "show_values(graph2, pr=\"\")\n",
    "show_values(graph3, pr=\"\")\n",
    "show_values(graph4, pr=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf338e",
   "metadata": {},
   "source": [
    "## 3. Jason Agent Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jason_metrics_csv = pd.read_csv(mlmas_additional_metrics_file, delimiter=';')\n",
    "\n",
    "final_columns = jason_metrics_csv.columns.values\n",
    "final_columns[6], final_columns[7] = final_columns[7], final_columns[6]\n",
    "\n",
    "jason_metrics_csv = jason_metrics_csv[final_columns]\n",
    "additional_metrics_columns = [\"Total Frames\", \n",
    "                              \"Front \\nCollision Avoidance\",\n",
    "                             \"Far Crossing \\nCollision Avoidance\",\n",
    "                             \"Close Crossing \\nCollision Avoidance\",\n",
    "                             \"Back \\nCollision Avoidance\",\n",
    "                             \"Traffic Light \\nGreen [Go]\",\n",
    "                             \"Traffic Light \\nSlowdown\",\n",
    "                             \"Traffic jam \\ninterference\"]\n",
    "jason_metrics_csv.columns = additional_metrics_columns\n",
    "\n",
    "print(\"== The additional Jason Agent metrics in each scenario\")\n",
    "jason_metrics_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb57e6",
   "metadata": {},
   "source": [
    "## Summary of the additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801942f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jason_metrics_csv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb707fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 17}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "mean_percentage = round(jason_metrics_csv.describe()*100,1)\n",
    "mean_percentage = mean_percentage[additional_metrics_columns[1:]][1:2].values[0]\n",
    "final_df_s = pd.DataFrame({'Interference_Percentage': mean_percentage\n",
    "                  , 'Interference_Type': additional_metrics_columns[1:]}) \n",
    "\n",
    "final_df_s.index = final_df_s.Interference_Type\n",
    "final_df_s = final_df_s.sort_values(by=[\"Interference_Percentage\"])\n",
    "graph = final_df_s.plot.barh(figsize=(13,13))\n",
    "graph.set_title(\"Average Jason Agent Interference Percentage of the Whole Evaluation Time\")\n",
    "show_values(graph, is_vertical= False)\n",
    "\n",
    "print(\"Total Average Agent Interferance Percentage: (%.1f%s)\"%(mean_percentage.sum(),\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed448c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_df = pd.DataFrame({\"Models\":[\"ML-Model\",\"(BDI) Agent\"], \"Contribution\": [100 - mean_percentage.sum(), mean_percentage.sum()]})\n",
    "total_df.index = total_df.Models\n",
    "\n",
    "plot = total_df.plot.pie(y='Contribution', title=\"The ML-Model vs (BDI) Agent Contribution\", legend=False, \\\n",
    "                   autopct='%1.1f%%', explode=(0, 0.1), \\\n",
    "                   shadow=True, \n",
    "                         startangle=-4,figsize=(8, 8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
